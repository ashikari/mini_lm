# mini_lm
Basic repository to learn how to pre-train an LLM

## Dataset Selection

For this project, we have chosen to use the BabyLM dataset instead of the more commonly used WikiText-103 dataset. This decision is based on several key advantages:

1. **Developmentally Plausible Training Data**: BabyLM provides a dataset that more closely mirrors the linguistic environment of a child's development, making it particularly suitable for studying language acquisition and model development.

2. **Controlled Vocabulary**: The dataset is carefully curated to include developmentally appropriate language patterns and vocabulary, which can lead to more interpretable model behavior.

3. **Research Focus**: The BabyLM dataset is specifically designed for studying language acquisition and model development, making it ideal for educational and research purposes.

4. **Quality and Diversity**: While maintaining a developmentally appropriate scope, the dataset still provides sufficient diversity and quality for effective language model training.

For more details about the BabyLM dataset and its design principles, please refer to the original paper: [BabyLM: A Developmentally Plausible Training Corpus for Language Models](https://arxiv.org/pdf/2301.11796v1)
